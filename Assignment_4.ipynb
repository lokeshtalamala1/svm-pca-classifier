{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# CS5691: Programming Assignment IV - SVM and PCA\n\nThis notebook provides a complete solution for Programming Assignment IV, focusing on Support Vector Machines (SVM) with various kernels and Principal Component Analysis (PCA) for different datasets.\n\n**Note**: This notebook assumes the required datasets (`Dataset1`, `Dataset2`, and `Dataset3`) are located in a folder named `Team15` in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necessary imports for the assignment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper_functions",
   "metadata": {},
   "source": [
    "## Helper Functions\n\nThese functions are used to print accuracy tables, plot confusion matrices, and visualize decision regions for the SVM classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracies(accuracies, model_type, c_values, kernel_param_name=None, kernel_params=None):\n",
    "    \"\"\"\n",
    "    Prints a formatted table of accuracies for different models and hyperparameters.\n",
    "    \"\"\"\n",
    "    if kernel_param_name and kernel_params:\n",
    "        data = {\n",
    "            kernel_param_name: kernel_params * len(c_values),\n",
    "            'C Value': c_values * len(kernel_params),\n",
    "            'Train Accuracy': [acc[0] for acc in accuracies],\n",
    "            'Validation Accuracy': [acc[1] for acc in accuracies],\n",
    "            'Test Accuracy': [acc[2] for acc in accuracies]\n",
    "        }\n",
    "        accuracy_df = pd.DataFrame(data)\n",
    "        print(f\"Accuracy Table for {model_type}:\")\n",
    "        print(tabulate(accuracy_df, headers='keys', tablefmt='grid'))\n",
    "    else:\n",
    "        data = {\n",
    "            'C Value': c_values,\n",
    "            'Train Accuracy': [acc[0] for acc in accuracies],\n",
    "            'Validation Accuracy': [acc[1] for acc in accuracies],\n",
    "            'Test Accuracy': [acc[2] for acc in accuracies]\n",
    "        }\n",
    "        accuracy_df = pd.DataFrame(data)\n",
    "        print(f\"Accuracy Table for {model_type}:\")\n",
    "        print(tabulate(accuracy_df, headers='keys', tablefmt='grid'))\n",
    "        \n",
    "def plot_confusion_matrix(cm, title, labels):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using seaborn.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=[f'Predicted {i}' for i in labels],\n",
    "                yticklabels=[f'Actual {i}' for i in labels])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_decision_region(X, y, model, title):\n",
    "    \"\"\"\n",
    "    Plots the decision regions and support vectors for a given SVM model.\n",
    "    \"\"\"\n",
    "    # Create a mesh grid for plotting decision regions\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    # Create the meshgrid with specified step size\n",
    "    xx, yy = np.meshgrid(np.arange(x1_min, x1_max, 0.01), np.arange(x2_min, x2_max, 0.01))\n",
    "\n",
    "    if isinstance(model, LinearSVC):\n",
    "        # For LinearSVC, we need to use decision_function on each one-vs-rest classifier\n",
    "        Z_decision = np.zeros((xx.ravel().shape[0], len(model.classes_)))\n",
    "        for i, classifier in enumerate(model.estimators_):\n",
    "            Z_decision[:, i] = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = np.argmax(Z_decision, axis=1)\n",
    "    else:\n",
    "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot decision boundaries and support vectors\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    classes = np.unique(y)\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(classes)))\n",
    "    light_colors = [plt.cm.rainbow(i, alpha=0.3) for i in np.linspace(0, 1, len(classes))]\n",
    "\n",
    "    cmap_light = ListedColormap(light_colors)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light, levels=np.arange(len(classes) + 1) - 0.5)\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(colors), s=20)\n",
    "\n",
    "    if hasattr(model, 'support_vectors_'):\n",
    "        plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "                    facecolors='none', edgecolors='k', s=80, marker='o', label='Support Vectors')\n",
    "\n",
    "    if isinstance(model, LinearSVC):\n",
    "        w = model.coef_\n",
    "        b = model.intercept_\n",
    "        for i in range(len(w)):\n",
    "            x_vals = np.linspace(x1_min, x1_max, 100)\n",
    "            y_vals = -(w[i, 0] * x_vals + b[i]) / w[i, 1]\n",
    "            plt.plot(x_vals, y_vals, 'k-')  # Decision boundary\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_decision_region_polynomial(X, y, model, title):\n",
    "    \"\"\"\n",
    "    Plots decision regions and bounded/unbounded support vectors for polynomial kernel SVM.\n",
    "    \"\"\"\n",
    "    # Create a mesh grid for plotting decision regions\n",
    "    x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    x2_min, x2_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x1_min, x1_max, 0.02), np.arange(x2_min, x2_max, 0.02))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n",
    "    plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.3)\n",
    "\n",
    "    # Plot the data points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['#FF0000', '#00FF00']), edgecolors='k', s=30)\n",
    "\n",
    "    # Identify bounded and unbounded support vectors\n",
    "    sv = model.support_vectors_\n",
    "    dual_coef = model.dual_coef_.ravel()\n",
    "    \n",
    "    bounded_sv_indices = np.where(np.abs(dual_coef) < model.C)[0]\n",
    "    unbounded_sv_indices = np.where(np.abs(dual_coef) >= model.C)[0]\n",
    "\n",
    "    bounded_sv = sv[bounded_sv_indices]\n",
    "    unbounded_sv = sv[unbounded_sv_indices]\n",
    "\n",
    "    # Plot support vectors\n",
    "    plt.scatter(unbounded_sv[:, 0], unbounded_sv[:, 1], s=100, facecolors='none', edgecolors='b', marker='o', label='Unbounded Support Vectors')\n",
    "    plt.scatter(bounded_sv[:, 0], bounded_sv[:, 1], s=100, facecolors='none', edgecolors='r', marker='o', label='Bounded Support Vectors')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_decision_region_gaussian(X, y, model, title, best_gamma, best_c):\n",
    "    \"\"\"\n",
    "    Plots decision regions and bounded/unbounded support vectors for Gaussian kernel SVM.\n",
    "    \"\"\"\n",
    "    # Create a mesh grid for plotting decision regions\n",
    "    x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    x2_min, x2_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x1_min, x1_max, 0.02), np.arange(x2_min, x2_max, 0.02))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n",
    "    plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.3)\n",
    "\n",
    "    # Plot the data points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['#FF0000', '#00FF00']), edgecolors='k', s=30)\n",
    "\n",
    "    # Identify bounded and unbounded support vectors\n",
    "    sv = model.support_vectors_\n",
    "    dual_coef = model.dual_coef_.ravel()\n",
    "    \n",
    "    bounded_sv_indices = np.where(np.abs(dual_coef) < best_c)[0]\n",
    "    unbounded_sv_indices = np.where(np.abs(dual_coef) >= best_c)[0]\n",
    "\n",
    "    bounded_sv = sv[bounded_sv_indices]\n",
    "    unbounded_sv = sv[unbounded_sv_indices]\n",
    "\n",
    "    # Plot support vectors\n",
    "    plt.scatter(unbounded_sv[:, 0], unbounded_sv[:, 1], s=100, facecolors='none', edgecolors='b', marker='o', label='Unbounded Support Vectors')\n",
    "    plt.scatter(bounded_sv[:, 0], bounded_sv[:, 1], s=100, facecolors='none', edgecolors='r', marker='o', label='Bounded Support Vectors')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_support_vector_percentages(model, X_train, y_train, C):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of bounded and unbounded support vectors.\n",
    "    \"\"\"\n",
    "    dual_coefs = model.dual_coef_.ravel()\n",
    "    n_total_sv = len(dual_coefs)\n",
    "    \n",
    "    # Check if the kernel is not linear, as dual_coefs are different for linear kernels\n",
    "    if not isinstance(model, LinearSVC):\n",
    "        # For non-linear kernels, bounded SVs are those with alpha < C\n",
    "        bounded_sv_count = np.sum(np.abs(dual_coefs) < C)\n",
    "        unbounded_sv_count = np.sum(np.abs(dual_coefs) >= C)\n",
    "    else:\n",
    "        # For LinearSVC, support vectors are not directly a single value of alpha\n",
    "        # This part of the function is simplified as it's not directly requested in the assignment\n",
    "        bounded_sv_count = np.sum(np.abs(dual_coefs) < C)\n",
    "        unbounded_sv_count = np.sum(np.abs(dual_coefs) >= C)\n",
    "    \n",
    "    total_examples = len(X_train)\n",
    "    percentage_bounded = (bounded_sv_count / total_examples) * 100\n",
    "    percentage_unbounded = (unbounded_sv_count / total_examples) * 100\n",
    "    \n",
    "    return percentage_bounded, percentage_unbounded\n",
    "\n",
    "def plot_cumulative_variance(pca, title):\n",
    "    \"\"\"\n",
    "    Plots the cumulative variance vs. the number of principal components.\n",
    "    \"\"\"\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_1",
   "metadata": {},
   "source": [
    "## Exercise 1: Linear Kernel SVM for Dataset 1\n\nThis exercise involves training a linear kernel SVM on Dataset 1, which is linearly separable. We'll use a `LinearSVC` classifier and evaluate its performance using accuracy and confusion matrices. The decision region plot will also be visualized, marking the support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise_1_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset 1\n",
    "train_data_1 = pd.read_csv('./Team15/Dataset-1/train.csv')\n",
    "val_data_1 = pd.read_csv('./Team15/Dataset-1/val.csv')\n",
    "test_data_1 = pd.read_csv('./Team15/Dataset-1/test.csv')\n",
    "\n",
    "X_train_1 = train_data_1[['x1', 'x2']].values\n",
    "y_train_1 = train_data_1['label'].values\n",
    "X_val_1 = val_data_1[['x1', 'x2']].values\n",
    "y_val_1 = val_data_1['label'].values\n",
    "X_test_1 = test_data_1[['x1', 'x2']].values\n",
    "y_test_1 = test_data_1['label'].values\n",
    "\n",
    "# Define the C values to try\n",
    "c_values = [1, 10, 100]\n",
    "accuracies = []\n",
    "\n",
    "for c in c_values:\n",
    "    # Create a pipeline with StandardScaler and LinearSVC\n",
    "    svm_linear = make_pipeline(StandardScaler(), LinearSVC(C=c, max_iter=20000, multi_class='ovr', random_state=0))\n",
    "    svm_linear.fit(X_train_1, y_train_1)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_accuracy = svm_linear.score(X_train_1, y_train_1)\n",
    "    val_accuracy = svm_linear.score(X_val_1, y_val_1)\n",
    "    test_accuracy = svm_linear.score(X_test_1, y_test_1)\n",
    "    accuracies.append([train_accuracy, val_accuracy, test_accuracy])\n",
    "    \n",
    "    # Find the best C value based on validation accuracy\n",
    "    best_c_index = np.argmax([acc[1] for acc in accuracies])\n",
    "    best_c = c_values[best_c_index]\n",
    "    best_model = make_pipeline(StandardScaler(), LinearSVC(C=best_c, max_iter=20000, multi_class='ovr', random_state=0))\n",
    "    best_model.fit(X_train_1, y_train_1)\n",
    "\n",
    "# Print accuracy table\n",
    "print_accuracies(accuracies, \"Linear Kernel SVM (Dataset 1)\", c_values)\n",
    "\n",
    "# Evaluate the best model\n",
    "y_train_pred_1 = best_model.predict(X_train_1)\n",
    "y_test_pred_1 = best_model.predict(X_test_1)\n",
    "\n",
    "train_cm_1 = confusion_matrix(y_train_1, y_train_pred_1)\n",
    "test_cm_1 = confusion_matrix(y_test_1, y_test_pred_1)\n",
    "\n",
    "print(\"\\nBest C value: \", best_c)\n",
    "plot_confusion_matrix(train_cm_1, 'Training Confusion Matrix (Dataset 1)', np.unique(y_train_1))\n",
    "plot_confusion_matrix(test_cm_1, 'Testing Confusion Matrix (Dataset 1)', np.unique(y_test_1))\n",
    "\n",
    "# Plot decision region for the best model\n",
    "plot_decision_region(X_train_1, y_train_1, best_model.named_steps['linearsvc'],\n",
    "                     f'Decision Regions for Best Linear SVM (C={best_c})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_2",
   "metadata": {},
   "source": [
    "## Exercise 2: Polynomial and Gaussian Kernel SVM for Dataset 2\n\nThis exercise trains SVMs with polynomial and Gaussian kernels on Dataset 2, which is non-linearly separable. We will evaluate different hyperparameters (`C` and `degree`/`gamma`) and select the best performing model based on validation accuracy. The decision regions and support vectors for the best models will be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise_2_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset 2\n",
    "train_data_2 = pd.read_csv('./Team15/Dataset-2/train.csv')\n",
    "val_data_2 = pd.read_csv('./Team15/Dataset-2/val.csv')\n",
    "test_data_2 = pd.read_csv('./Team15/Dataset-2/test.csv')\n",
    "\n",
    "X_train_2 = train_data_2[['x1', 'x2']].values\n",
    "y_train_2 = train_data_2['output'].values\n",
    "X_val_2 = val_data_2[['x1', 'x2']].values\n",
    "y_val_2 = val_data_2['output'].values\n",
    "X_test_2 = test_data_2[['x1', 'x2']].values\n",
    "y_test_2 = test_data_2['output'].values\n",
    "\n",
    "# Grid search for Polynomial Kernel\n",
    "c_values = [1, 10, 100]\n",
    "degree_values = [2, 3, 4, 5]\n",
    "poly_accuracies = []\n",
    "best_poly_accuracy = 0\n",
    "best_poly_params = {}\n",
    "\n",
    "for c in c_values:\n",
    "    for degree in degree_values:\n",
    "        svm_poly = make_pipeline(StandardScaler(), SVC(kernel='poly', C=c, degree=degree))\n",
    "        svm_poly.fit(X_train_2, y_train_2)\n",
    "        \n",
    "        train_accuracy = svm_poly.score(X_train_2, y_train_2)\n",
    "        val_accuracy = svm_poly.score(X_val_2, y_val_2)\n",
    "        test_accuracy = svm_poly.score(X_test_2, y_test_2)\n",
    "        poly_accuracies.append([train_accuracy, val_accuracy, test_accuracy])\n",
    "        \n",
    "        if val_accuracy > best_poly_accuracy:\n",
    "            best_poly_accuracy = val_accuracy\n",
    "            best_poly_params = {'C': c, 'degree': degree}\n",
    "            best_poly_model = svm_poly\n",
    "\n",
    "print(\"Polynomial Kernel SVM Accuracies:\")\n",
    "print_accuracies(poly_accuracies, \"Polynomial Kernel SVM (Dataset 2)\", c_values * len(degree_values), \"Degree\", degree_values)\n",
    "print(f\"\\nBest Polynomial Kernel SVM parameters: {best_poly_params} with validation accuracy: {best_poly_accuracy:.4f}\")\n",
    "\n",
    "# Plot decision region for the best polynomial model\n",
    "if 'best_poly_model' in locals():\n",
    "    plot_decision_region_polynomial(X_train_2, y_train_2, best_poly_model.named_steps['svc'],\n",
    "                                    f\"Polynomial Kernel SVM (C={best_poly_params['C']}, Degree={best_poly_params['degree']})\")\n",
    "    \n",
    "# Grid search for Gaussian Kernel (RBF)\n",
    "gamma_values = [0.1, 1, 10, 100]\n",
    "rbf_accuracies = []\n",
    "best_rbf_accuracy = 0\n",
    "best_rbf_params = {}\n",
    "\n",
    "for c in c_values:\n",
    "    for gamma in gamma_values:\n",
    "        svm_rbf = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=c, gamma=gamma))\n",
    "        svm_rbf.fit(X_train_2, y_train_2)\n",
    "\n",
    "        train_accuracy = svm_rbf.score(X_train_2, y_train_2)\n",
    "        val_accuracy = svm_rbf.score(X_val_2, y_val_2)\n",
    "        test_accuracy = svm_rbf.score(X_test_2, y_test_2)\n",
    "        rbf_accuracies.append([train_accuracy, val_accuracy, test_accuracy])\n",
    "\n",
    "        if val_accuracy > best_rbf_accuracy:\n",
    "            best_rbf_accuracy = val_accuracy\n",
    "            best_rbf_params = {'C': c, 'gamma': gamma}\n",
    "            best_rbf_model = svm_rbf\n",
    "\n",
    "print(\"\\nGaussian Kernel SVM Accuracies:\")\n",
    "print_accuracies(rbf_accuracies, \"Gaussian Kernel SVM (Dataset 2)\", c_values * len(gamma_values), \"Gamma\", gamma_values)\n",
    "print(f\"\\nBest Gaussian Kernel SVM parameters: {best_rbf_params} with validation accuracy: {best_rbf_accuracy:.4f}\")\n",
    "\n",
    "# Plot decision region for the best Gaussian model\n",
    "if 'best_rbf_model' in locals():\n",
    "    plot_decision_region_gaussian(X_train_2, y_train_2, best_rbf_model.named_steps['svc'],\n",
    "                                  f\"Gaussian Kernel SVM (C={best_rbf_params['C']}, Gamma={best_rbf_params['gamma']})\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_3",
   "metadata": {},
   "source": [
    "## Exercise 3: Polynomial and Gaussian Kernel SVM for Dataset 3\n\nThis exercise repeats the grid search for SVMs with polynomial and Gaussian kernels, but this time on the image data from Dataset 3. The best model will be selected based on validation accuracy, and its performance will be analyzed with confusion matrices and support vector percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise_3_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset 3\n",
    "train_data_3 = pd.read_csv('./Team15/Dataset-3/train.csv')\n",
    "val_data_3 = pd.read_csv('./Team15/Dataset-3/val.csv')\n",
    "test_data_3 = pd.read_csv('./Team15/Dataset-3/test.csv')\n",
    "\n",
    "X_train_3 = train_data_3.drop('output', axis=1).values\n",
    "y_train_3 = train_data_3['output'].values\n",
    "X_val_3 = val_data_3.drop('output', axis=1).values\n",
    "y_val_3 = val_data_3['output'].values\n",
    "X_test_3 = test_data_3.drop('output', axis=1).values\n",
    "y_test_3 = test_data_3['output'].values\n",
    "\n",
    "# Grid search for Polynomial Kernel\n",
    "c_values = [1, 10, 100]\n",
    "degree_values = [2, 3, 4, 5]\n",
    "poly_accuracies_3 = []\n",
    "best_poly_accuracy_3 = 0\n",
    "best_poly_params_3 = {}\n",
    "\n",
    "for c in c_values:\n",
    "    for degree in degree_values:\n",
    "        svm_poly_3 = make_pipeline(StandardScaler(), SVC(kernel='poly', C=c, degree=degree))\n",
    "        svm_poly_3.fit(X_train_3, y_train_3)\n",
    "        \n",
    "        train_accuracy = svm_poly_3.score(X_train_3, y_train_3)\n",
    "        val_accuracy = svm_poly_3.score(X_val_3, y_val_3)\n",
    "        test_accuracy = svm_poly_3.score(X_test_3, y_test_3)\n",
    "        poly_accuracies_3.append([train_accuracy, val_accuracy, test_accuracy])\n",
    "        \n",
    "        if val_accuracy > best_poly_accuracy_3:\n",
    "            best_poly_accuracy_3 = val_accuracy\n",
    "            best_poly_params_3 = {'C': c, 'degree': degree}\n",
    "            best_poly_model_3 = svm_poly_3\n",
    "\n",
    "print(\"Polynomial Kernel SVM Accuracies (Dataset 3):\")\n",
    "print_accuracies(poly_accuracies_3, \"Polynomial Kernel SVM\", c_values, \"Degree\", degree_values)\n",
    "print(f\"\\nBest Polynomial Kernel SVM parameters: {best_poly_params_3} with validation accuracy: {best_poly_accuracy_3:.4f}\")\n",
    "\n",
    "if 'best_poly_model_3' in locals():\n",
    "    y_train_pred = best_poly_model_3.predict(X_train_3)\n",
    "    y_test_pred = best_poly_model_3.predict(X_test_3)\n",
    "    train_cm = confusion_matrix(y_train_3, y_train_pred)\n",
    "    test_cm = confusion_matrix(y_test_3, y_test_pred)\n",
    "    plot_confusion_matrix(train_cm, 'Training Confusion Matrix (Dataset 3)', np.unique(y_train_3))\n",
    "    plot_confusion_matrix(test_cm, 'Testing Confusion Matrix (Dataset 3)', np.unique(y_test_3))\n",
    "\n",
    "    # Calculate and print support vector percentages\n",
    "    percentage_bounded, percentage_unbounded = calculate_support_vector_percentages(\n",
    "        best_poly_model_3.named_steps['svc'], X_train_3, y_train_3, best_poly_params_3['C']\n",
    "    )\n",
    "    print(f\"\\nPercentage of Bounded Support Vectors (Polynomial): {percentage_bounded:.2f}%\")\n",
    "    print(f\"Percentage of Unbounded Support Vectors (Polynomial): {percentage_unbounded:.2f}%\")\n",
    "\n",
    "# Grid search for Gaussian Kernel (RBF)\n",
    "gamma_values = [0.1, 1, 10, 100]\n",
    "rbf_accuracies_3 = []\n",
    "best_rbf_accuracy_3 = 0\n",
    "best_rbf_params_3 = {}\n",
    "\n",
    "for c in c_values:\n",
    "    for gamma in gamma_values:\n",
    "        svm_rbf_3 = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=c, gamma=gamma))\n",
    "        svm_rbf_3.fit(X_train_3, y_train_3)\n",
    "\n",
    "        train_accuracy = svm_rbf_3.score(X_train_3, y_train_3)\n",
    "        val_accuracy = svm_rbf_3.score(X_val_3, y_val_3)\n",
    "        test_accuracy = svm_rbf_3.score(X_test_3, y_test_3)\n",
    "        rbf_accuracies_3.append([train_accuracy, val_accuracy, test_accuracy])\n",
    "\n",
    "        if val_accuracy > best_rbf_accuracy_3:\n",
    "            best_rbf_accuracy_3 = val_accuracy\n",
    "            best_rbf_params_3 = {'C': c, 'gamma': gamma}\n",
    "            best_rbf_model_3 = svm_rbf_3\n",
    "            \n",
    "print(\"\\nGaussian Kernel SVM Accuracies (Dataset 3):\")\n",
    "print_accuracies(rbf_accuracies_3, \"Gaussian Kernel SVM\", c_values, \"Gamma\", gamma_values)\n",
    "print(f\"\\nBest Gaussian Kernel SVM parameters: {best_rbf_params_3} with validation accuracy: {best_rbf_accuracy_3:.4f}\")\n",
    "\n",
    "if 'best_rbf_model_3' in locals():\n",
    "    y_train_pred = best_rbf_model_3.predict(X_train_3)\n",
    "    y_test_pred = best_rbf_model_3.predict(X_test_3)\n",
    "    train_cm = confusion_matrix(y_train_3, y_train_pred)\n",
    "    test_cm = confusion_matrix(y_test_3, y_test_pred)\n",
    "    plot_confusion_matrix(train_cm, 'Training Confusion Matrix (Dataset 3)', np.unique(y_train_3))\n",
    "    plot_confusion_matrix(test_cm, 'Testing Confusion Matrix (Dataset 3)', np.unique(y_test_3))\n",
    "\n",
    "    # Calculate and print support vector percentages\n",
    "    percentage_bounded, percentage_unbounded = calculate_support_vector_percentages(\n",
    "        best_rbf_model_3.named_steps['svc'], X_train_3, y_train_3, best_rbf_params_3['C']\n",
    "    )\n",
    "    print(f\"\\nPercentage of Bounded Support Vectors (Gaussian): {percentage_bounded:.2f}%\")\n",
    "    print(f\"Percentage of Unbounded Support Vectors (Gaussian): {percentage_unbounded:.2f}%\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_4",
   "metadata": {},
   "source": [
    "## Exercise 4: PCA for Dataset 3\n\nThis exercise performs Principal Component Analysis on Dataset 3 (image data). The goal is to determine a suitable reduced dimension by plotting the cumulative variance vs. the number of principal components. This will help in identifying how many components are needed to explain a significant portion of the data's variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise_4_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset 3\n",
    "train_data_3 = pd.read_csv('./Team15/Dataset-3/train.csv')\n",
    "X_train_3 = train_data_3.drop('output', axis=1).values\n",
    "\n",
    "# Perform PCA on the scaled data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_3)\n",
    "pca = PCA(n_components=X_train_scaled.shape[1])\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "# Plot cumulative variance\n",
    "plot_cumulative_variance(pca, 'Cumulative Variance vs. Number of Principal Components')\n",
    "\n",
    "# Determine a suitable reduced dimension\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "n_components_95_percent = np.where(cumulative_variance >= 0.95)[0][0] + 1\n",
    "print(f\"Number of components to explain at least 95% of the variance: {n_components_95_percent}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_5",
   "metadata": {},
   "source": [
    "## Exercise 5: Classifiers on PCA-reduced Dataset 3\n\nThis exercise uses the reduced-dimension representation of Dataset 3 (from the previous exercise) as input for three different classifiers: GMM, MLFFNN, and the best-performing SVM from Exercise 3. The configurations for GMM and MLFFNN are the same as in Assignment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise_5_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset 3\n",
    "train_data_3 = pd.read_csv('./Team15/Dataset-3/train.csv')\n",
    "val_data_3 = pd.read_csv('./Team15/Dataset-3/val.csv')\n",
    "test_data_3 = pd.read_csv('./Team15/Dataset-3/test.csv')\n",
    "\n",
    "X_train_3 = train_data_3.drop('output', axis=1).values\n",
    "y_train_3 = train_data_3['output'].values\n",
    "X_val_3 = val_data_3.drop('output', axis=1).values\n",
    "y_val_3 = val_data_3['output'].values\n",
    "X_test_3 = test_data_3.drop('output', axis=1).values\n",
    "y_test_3 = test_data_3['output'].values\n",
    "\n",
    "# Reduce dimension using PCA\n",
    "n_components = 4 # Using the same value as in the original report\n",
    "pca = make_pipeline(StandardScaler(), PCA(n_components=n_components))\n",
    "X_train_pca = pca.fit_transform(X_train_3)\n",
    "X_val_pca = pca.transform(X_val_3)\n",
    "X_test_pca = pca.transform(X_test_3)\n",
    "\n",
    "# 5a. GMM based classifier\n",
    "print(\"\\n--- GMM based classifier ---\")\n",
    "best_gmm = None\n",
    "best_accuracy = 0\n",
    "for cov_type in ['full', 'diag']:\n",
    "    for n_components_gmm in [1, 3, 5, 7]:\n",
    "        gmm_classifier = make_pipeline(StandardScaler(), GaussianMixture(n_components=n_components_gmm, \n",
    "                                                                         covariance_type=cov_type, random_state=0))\n",
    "        gmm_classifier.fit(X_train_3, y_train_3)\n",
    "        \n",
    "        val_accuracy = gmm_classifier.score(X_val_3, y_val_3)\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_gmm = gmm_classifier\n",
    "\n",
    "train_accuracy = best_gmm.score(X_train_3, y_train_3)\n",
    "test_accuracy = best_gmm.score(X_test_3, y_test_3)\n",
    "\n",
    "print(f\"Best GMM Classifier Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Best GMM Classifier Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 5b. MLFFNN based classifier\n",
    "print(\"\\n--- MLFFNN based classifier ---\")\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.Tensor(X_train_pca)\n",
    "y_train_tensor = torch.LongTensor(y_train_3)\n",
    "X_test_tensor = torch.Tensor(X_test_pca)\n",
    "y_test_tensor = torch.LongTensor(y_test_3)\n",
    "\n",
    "class MLFFNN_PCA(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLFFNN_PCA, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 12)\n",
    "        self.fc2 = nn.Linear(12, 8)\n",
    "        self.fc3 = nn.Linear(8, 5) # 5 classes for Dataset 3\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.beta = 1.0\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tanh(self.beta * self.fc1(x))\n",
    "        x = self.tanh(self.beta * self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "model_pca = MLFFNN_PCA(input_dim=n_components)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_pca.parameters(), lr=0.7, momentum=0.9)\n",
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model_pca(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "with torch.no_grad():\n",
    "    model_pca.eval()\n",
    "    train_outputs = model_pca(X_train_tensor)\n",
    "    test_outputs = model_pca(X_test_tensor)\n",
    "    train_pred = torch.argmax(train_outputs, dim=1)\n",
    "    test_pred = torch.argmax(test_outputs, dim=1)\n",
    "    \n",
    "train_acc = accuracy_score(y_train_3, train_pred.numpy())\n",
    "test_acc = accuracy_score(y_test_3, test_pred.numpy())\n",
    "print(f\"MLFFNN Training Accuracy (PCA): {train_acc:.4f}\")\n",
    "print(f\"MLFFNN Testing Accuracy (PCA): {test_acc:.4f}\")\n",
    "\n",
    "# 5c. Best SVM classifier from Exercise 3\n",
    "print(\"\\n--- Best SVM classifier ---\")\n",
    "# Assuming the best SVM model from Exercise 3 is saved as best_rbf_model_3\n",
    "if 'best_rbf_model_3' in locals():\n",
    "    best_svm = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=best_rbf_params_3['C'], gamma=best_rbf_params_3['gamma']))\n",
    "    best_svm.fit(X_train_3, y_train_3)\n",
    "\n",
    "    train_accuracy = best_svm.score(X_train_3, y_train_3)\n",
    "    test_accuracy = best_svm.score(X_test_3, y_test_3)\n",
    "\n",
    "    print(f\"Best SVM Training Accuracy (PCA): {train_accuracy:.4f}\")\n",
    "    print(f\"Best SVM Testing Accuracy (PCA): {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
